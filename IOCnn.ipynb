{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgyYOQNn-Aue",
        "outputId": "f9059151-d4e7-4ede-bea0-064e2234759f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "!pip install torchsummary\n",
        "\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "import tensorflow\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torchvision import models\n",
        "import pickle\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# MLP with three hidden layers and 800 nodes in each layer with batch normalization between every layer, and itâ€™s activation in all hidden layers. ReLU and ELU are used as activations for NN and IOC respectively, and softmax is used in the last layer. We use Adam optimizer.\n",
        "\n",
        "class MLP(nn.Module ):\n",
        "    def __init__(self , IOCflag = False):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(784*3, 800)\n",
        "        self.fc2 = nn.Linear(800, 800)\n",
        "        self.fc3 = nn.Linear(800, 800)\n",
        "        self.fc4 = nn.Linear(800, 10)\n",
        "        self.bn1 = nn.BatchNorm1d(800)\n",
        "        self.bn2 = nn.BatchNorm1d(800)  \n",
        "        self.bn3 = nn.BatchNorm1d(800)\n",
        "        self.IOCflag = IOCflag\n",
        "        self.activation = nn.ReLU()\n",
        "        if self.IOCflag:\n",
        "            self.activation = nn.ELU()\n",
        "            \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 3*784)\n",
        "        x = self.bn1(self.activation(self.fc1(x)))\n",
        "        x = self.bn2(self.activation(self.fc2(x)))\n",
        "        x = self.bn3(self.activation(self.fc3(x)))\n",
        "        x = F.softmax(self.fc4(x), dim=1)\n",
        "        # x = nn.fc4(self.activation(x))\n",
        "        # x = self.fc4(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "J2tjiapMFqrF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR10 dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(),transforms.CenterCrop(28),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data',train=True,download=True,transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data',train=False,download=True,transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=128,shuffle=True,num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=128,shuffle=False,num_workers=2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32r8cp6QLzAt",
        "outputId": "30c01bc3-881f-4d62-cb1a-f1157d4c4d4c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR10 dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(),transforms.CenterCrop(28),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform)\n",
        "\n",
        "# for i in range(len(train_dataset)):\n",
        "#         train_dataset.data[i] = tf.image.per_image_standardization(train_dataset.data[i])\n",
        "# for i in range(len(test_dataset)):\n",
        "#         test_dataset.data[i] = tf.image.per_image_standardization(test_dataset.data[i])\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=64, shuffle=True, num_workers=2,pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=64, shuffle=False, num_workers=2,pin_memory=True)\n",
        "\n",
        "print('Number of training examples: ', train_loader.dataset.data.shape)\n",
        "print('Number of testing examples: ', test_loader.dataset.data.shape)\n",
        "print(len(train_loader))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsrGAsOQFu7G",
        "outputId": "81dd48db-7a90-4133-88fc-259ab0ff47e9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Number of training examples:  (50000, 32, 32, 3)\n",
            "Number of testing examples:  (10000, 32, 32, 3)\n",
            "782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "pbar = tqdm(total=len(train_loader))\n",
        "for idx, (data,label) in enumerate(train_loader):\n",
        "    print(idx,data.size(),label.size())\n",
        "    pbar.update(1)\n",
        "    break\n",
        "pbar.refresh()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPA_aSarJ0kW",
        "outputId": "b2f7f8c6-13ed-4da1-ee6e-503d50243167"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/782 [00:08<1:45:18,  8.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 torch.Size([64, 3, 28, 28]) torch.Size([64])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def LabelCorruption(percentage,myloader):\n",
        "    labels = myloader.dataset.targets[:]\n",
        "    data = myloader.dataset.data\n",
        "    num = int(len(labels)*percentage)\n",
        "    idxes = []\n",
        "    if percentage == 1:\n",
        "        idxes = list(range(len(labels)))\n",
        "    while len(idxes) < num:\n",
        "        idx = np.random.randint(0,len(labels))\n",
        "        if idx not in idxes:\n",
        "            idxes.append(idx)\n",
        "    for idx in idxes:\n",
        "        randomLabel = np.random.randint(0,10)\n",
        "        while randomLabel == labels[idx]:\n",
        "            randomLabel = np.random.randint(0,10)\n",
        "        labels[idx] = randomLabel\n",
        "    corrupted_dataset = torchvision.datasets.CIFAR10(root='./', train=True, download=True, transform=transform)\n",
        "    corrupted_dataset.data = data\n",
        "    corrupted_dataset.targets = labels\n",
        "    corrupted_loader = torch.utils.data.DataLoader(dataset=corrupted_dataset, batch_size=128, shuffle=True)\n",
        "    return corrupted_loader\n",
        "\n"
      ],
      "metadata": {
        "id": "aOVwkjANI-2B"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corrupted_trainloader = LabelCorruption(1,train_loader)"
      ],
      "metadata": {
        "id": "8czD-dyqf19K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chkcorruptionLabels(train_loader , corrupter_trainloder):\n",
        "    noOfIncorrectLabels = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        for j in range(len(labels)):\n",
        "            if labels[j] != corrupter_trainloder.dataset.targets[i*128+j]:\n",
        "                noOfIncorrectLabels += 1\n",
        "    return noOfIncorrectLabels\n",
        "\n",
        "print(chkcorruptionLabels(train_loader,corrupted_trainloader))"
      ],
      "metadata": {
        "id": "VCVCH6iyf31U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test(model , test_loader , criterion):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    running_loss = 0.0\n",
        "    running_correct = 0.0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(test_loader,0):\n",
        "            # print(len(data))\n",
        "            images, labels = data\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            running_correct += (predicted == labels).sum().item()\n",
        "    print(\"Test Accuracy\" , running_correct/total , \"Test Loss\" , running_loss/total)\n",
        "    return running_correct/total , running_loss/total\n",
        "\n",
        "\n",
        "\n",
        "def train(model , train_loader,test_loader,scheduler , optimizer , epochs , IOCflag = False):\n",
        "    model.train()\n",
        "    train_loss = []\n",
        "    train_acc = []\n",
        "    test_losslst = []\n",
        "    test_acclst = []\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for epoch in range(epochs):\n",
        "        correct = 0\n",
        "        running_loss = 0.0\n",
        "        running_correct = 0.0\n",
        "        total = 0\n",
        "        for batch_idx, data in enumerate(train_loader,0):\n",
        "            images, labels = data\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if IOCflag:\n",
        "              with torch.no_grad():\n",
        "\n",
        "                for layer in [model.fc2.weight, model.fc3.weight, model.fc4.weight]:\n",
        "                      layer[layer < 0] = torch.exp(layer[layer < 0] - 5)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            running_correct += (predicted == labels).sum().item()\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(\"Epoch\" , epoch , \"Batch\" , batch_idx , \"Loss\" , running_loss/total , \"Accuracy\" , running_correct/total)\n",
        "                train_loss.append(running_loss/total)\n",
        "                train_acc.append(running_correct/total)\n",
        "                test_acc , test_loss = test(model , test_loader , criterion)\n",
        "                test_losslst.append(test_loss)\n",
        "                test_acclst.append(test_acc)\n",
        "    scheduler.step()\n",
        "    \n",
        "    return train_loss , train_acc,test_losslst,test_acclst\n",
        "            \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A-96YhF0Ftzx"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MLPmodel = MLP()\n",
        "MLPmodel.cuda()\n",
        "optimizer = optim.Adam(MLPmodel.parameters(), lr=0.0001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
        "\n",
        "MLPtrain_loss , MLPtrain_acc , MLPtest_loss , MLPtest_acc = train(MLPmodel , corrupted_trainloader,test_loader,scheduler , optimizer , 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_L3aSCCfKqm",
        "outputId": "36e1e8e1-7e5a-4b90-8dc1-212cf55f5b21"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Batch 0 Loss 0.017996549606323242 Accuracy 0.09375\n",
            "Test Accuracy 0.1031 Test Loss 0.018189794945716858\n",
            "Epoch 0 Batch 100 Loss 0.01798630249456014 Accuracy 0.10419245049504951\n",
            "Test Accuracy 0.0453 Test Loss 0.01825043845176697\n",
            "Epoch 0 Batch 200 Loss 0.01798382109559294 Accuracy 0.10607120646766169\n",
            "Test Accuracy 0.0388 Test Loss 0.01833628737926483\n",
            "Epoch 0 Batch 300 Loss 0.01798177931296865 Accuracy 0.10600083056478406\n",
            "Test Accuracy 0.0324 Test Loss 0.01839664614200592\n",
            "Epoch 1 Batch 0 Loss 0.017918886616826057 Accuracy 0.1484375\n",
            "Test Accuracy 0.0295 Test Loss 0.018424300384521485\n",
            "Epoch 1 Batch 100 Loss 0.017967571476751036 Accuracy 0.11130878712871287\n",
            "Test Accuracy 0.032 Test Loss 0.01845751130580902\n",
            "Epoch 1 Batch 200 Loss 0.017967371158857843 Accuracy 0.11326181592039801\n",
            "Test Accuracy 0.0303 Test Loss 0.01849134967327118\n",
            "Epoch 1 Batch 300 Loss 0.017966568828916233 Accuracy 0.11238579734219269\n",
            "Test Accuracy 0.0323 Test Loss 0.018483714890480042\n",
            "Epoch 2 Batch 0 Loss 0.0179965291172266 Accuracy 0.1171875\n",
            "Test Accuracy 0.0293 Test Loss 0.018504248046875\n",
            "Epoch 2 Batch 100 Loss 0.017953338562557014 Accuracy 0.11827042079207921\n",
            "Test Accuracy 0.0267 Test Loss 0.018505702185630798\n",
            "Epoch 2 Batch 200 Loss 0.01795160311705141 Accuracy 0.11909203980099503\n",
            "Test Accuracy 0.0268 Test Loss 0.01854042615890503\n",
            "Epoch 2 Batch 300 Loss 0.01794954570647688 Accuracy 0.11978301495016612\n",
            "Test Accuracy 0.0279 Test Loss 0.018567544436454773\n",
            "Epoch 3 Batch 0 Loss 0.017959561198949814 Accuracy 0.0703125\n",
            "Test Accuracy 0.033 Test Loss 0.018523302745819092\n",
            "Epoch 3 Batch 100 Loss 0.017933374842499742 Accuracy 0.12530940594059406\n",
            "Test Accuracy 0.0261 Test Loss 0.01857707977294922\n",
            "Epoch 3 Batch 200 Loss 0.017929929998976673 Accuracy 0.12465018656716417\n",
            "Test Accuracy 0.0323 Test Loss 0.018581083512306213\n",
            "Epoch 3 Batch 300 Loss 0.017929142303888583 Accuracy 0.12575269933554817\n",
            "Test Accuracy 0.0341 Test Loss 0.018568710899353028\n",
            "Epoch 4 Batch 0 Loss 0.017946120351552963 Accuracy 0.09375\n",
            "Test Accuracy 0.0323 Test Loss 0.018594997525215148\n",
            "Epoch 4 Batch 100 Loss 0.017885997135302808 Accuracy 0.13884591584158415\n",
            "Test Accuracy 0.0323 Test Loss 0.018675613117218017\n",
            "Epoch 4 Batch 200 Loss 0.01789787532159345 Accuracy 0.1363495024875622\n",
            "Test Accuracy 0.03 Test Loss 0.018629518032073973\n",
            "Epoch 4 Batch 300 Loss 0.017900697596098893 Accuracy 0.13597902823920266\n",
            "Test Accuracy 0.0288 Test Loss 0.018673798561096192\n",
            "Epoch 5 Batch 0 Loss 0.018009141087532043 Accuracy 0.1015625\n",
            "Test Accuracy 0.0403 Test Loss 0.018644994020462036\n",
            "Epoch 5 Batch 100 Loss 0.017858610866536007 Accuracy 0.14611695544554457\n",
            "Test Accuracy 0.0322 Test Loss 0.018682100772857667\n",
            "Epoch 5 Batch 200 Loss 0.017857510494577945 Accuracy 0.14525031094527363\n",
            "Test Accuracy 0.0303 Test Loss 0.01868805673122406\n",
            "Epoch 5 Batch 300 Loss 0.01786159137975536 Accuracy 0.14264950166112958\n",
            "Test Accuracy 0.028 Test Loss 0.01869062466621399\n",
            "Epoch 6 Batch 0 Loss 0.017796030268073082 Accuracy 0.1875\n",
            "Test Accuracy 0.0324 Test Loss 0.018718426728248597\n",
            "Epoch 6 Batch 100 Loss 0.017789451184101624 Accuracy 0.15965346534653466\n",
            "Test Accuracy 0.0384 Test Loss 0.01871836767196655\n",
            "Epoch 6 Batch 200 Loss 0.017790178328159437 Accuracy 0.15827114427860697\n",
            "Test Accuracy 0.0322 Test Loss 0.018738279175758363\n",
            "Epoch 6 Batch 300 Loss 0.017797165955617974 Accuracy 0.15622404485049834\n",
            "Test Accuracy 0.032 Test Loss 0.018731564831733702\n",
            "Epoch 7 Batch 0 Loss 0.017537465319037437 Accuracy 0.21875\n",
            "Test Accuracy 0.0316 Test Loss 0.01873732740879059\n",
            "Epoch 7 Batch 100 Loss 0.017703070959979944 Accuracy 0.17636138613861385\n",
            "Test Accuracy 0.0361 Test Loss 0.018754033946990968\n",
            "Epoch 7 Batch 200 Loss 0.01771819277374602 Accuracy 0.17269123134328357\n",
            "Test Accuracy 0.0374 Test Loss 0.01876642153263092\n",
            "Epoch 7 Batch 300 Loss 0.017719663135039053 Accuracy 0.17270556478405316\n",
            "Test Accuracy 0.0386 Test Loss 0.018753062605857847\n",
            "Epoch 8 Batch 0 Loss 0.017762450501322746 Accuracy 0.1484375\n",
            "Test Accuracy 0.0331 Test Loss 0.018783057260513304\n",
            "Epoch 8 Batch 100 Loss 0.017589081521376525 Accuracy 0.19515779702970298\n",
            "Test Accuracy 0.0356 Test Loss 0.01882132852077484\n",
            "Epoch 8 Batch 200 Loss 0.017603629310406856 Accuracy 0.19057058457711443\n",
            "Test Accuracy 0.0359 Test Loss 0.01878993384838104\n",
            "Epoch 8 Batch 300 Loss 0.017618182056468982 Accuracy 0.18731831395348839\n",
            "Test Accuracy 0.0301 Test Loss 0.01883444764614105\n",
            "Epoch 9 Batch 0 Loss 0.017592309042811394 Accuracy 0.1953125\n",
            "Test Accuracy 0.0363 Test Loss 0.018799737644195555\n",
            "Epoch 9 Batch 100 Loss 0.01749711050329232 Accuracy 0.20753403465346534\n",
            "Test Accuracy 0.0365 Test Loss 0.018801879024505617\n",
            "Epoch 9 Batch 200 Loss 0.017489039380826167 Accuracy 0.2097714552238806\n",
            "Test Accuracy 0.0348 Test Loss 0.018831865429878233\n",
            "Epoch 9 Batch 300 Loss 0.01750299657549573 Accuracy 0.2071999584717608\n",
            "Test Accuracy 0.0331 Test Loss 0.018863720178604125\n",
            "Epoch 10 Batch 0 Loss 0.017322344705462456 Accuracy 0.25\n",
            "Test Accuracy 0.0295 Test Loss 0.018867531609535217\n",
            "Epoch 10 Batch 100 Loss 0.01733252542591331 Accuracy 0.23654084158415842\n",
            "Test Accuracy 0.0358 Test Loss 0.018866478395462037\n",
            "Epoch 10 Batch 200 Loss 0.017366367776817944 Accuracy 0.22959421641791045\n",
            "Test Accuracy 0.0381 Test Loss 0.018842394042015077\n",
            "Epoch 10 Batch 300 Loss 0.01737996260712907 Accuracy 0.22645867940199335\n",
            "Test Accuracy 0.0324 Test Loss 0.018884835481643675\n",
            "Epoch 11 Batch 0 Loss 0.01722215674817562 Accuracy 0.265625\n",
            "Test Accuracy 0.0384 Test Loss 0.01885463743209839\n",
            "Epoch 11 Batch 100 Loss 0.017199858672695585 Accuracy 0.2547184405940594\n",
            "Test Accuracy 0.0466 Test Loss 0.018808030462265014\n",
            "Epoch 11 Batch 200 Loss 0.017201335127673932 Accuracy 0.2532260572139303\n",
            "Test Accuracy 0.0377 Test Loss 0.01886683294773102\n",
            "Epoch 11 Batch 300 Loss 0.017215586115801058 Accuracy 0.2504152823920266\n",
            "Test Accuracy 0.0365 Test Loss 0.01888996846675873\n",
            "Epoch 12 Batch 0 Loss 0.017115814611315727 Accuracy 0.28125\n",
            "Test Accuracy 0.0365 Test Loss 0.018891810727119446\n",
            "Epoch 12 Batch 100 Loss 0.017036756445275674 Accuracy 0.27583539603960394\n",
            "Test Accuracy 0.0396 Test Loss 0.01885386004447937\n",
            "Epoch 12 Batch 200 Loss 0.017056114897502597 Accuracy 0.27409825870646765\n",
            "Test Accuracy 0.0376 Test Loss 0.018898406195640562\n",
            "Epoch 12 Batch 300 Loss 0.017061172158417116 Accuracy 0.2726588455149502\n",
            "Test Accuracy 0.0387 Test Loss 0.0188947740316391\n",
            "Epoch 13 Batch 0 Loss 0.016978062689304352 Accuracy 0.28125\n",
            "Test Accuracy 0.0374 Test Loss 0.018899303340911864\n",
            "Epoch 13 Batch 100 Loss 0.016905776086715188 Accuracy 0.2949412128712871\n",
            "Test Accuracy 0.0397 Test Loss 0.01887867407798767\n",
            "Epoch 13 Batch 200 Loss 0.016915701616981728 Accuracy 0.29326026119402987\n",
            "Test Accuracy 0.0391 Test Loss 0.018889691495895387\n",
            "Epoch 13 Batch 300 Loss 0.016936954933890077 Accuracy 0.28981519933554817\n",
            "Test Accuracy 0.0362 Test Loss 0.01893221824169159\n",
            "Epoch 14 Batch 0 Loss 0.017097465693950653 Accuracy 0.2734375\n",
            "Test Accuracy 0.0344 Test Loss 0.018939443254470827\n",
            "Epoch 14 Batch 100 Loss 0.016751424732184647 Accuracy 0.3165996287128713\n",
            "Test Accuracy 0.0381 Test Loss 0.018907519841194154\n",
            "Epoch 14 Batch 200 Loss 0.01675683459202152 Accuracy 0.31665889303482586\n",
            "Test Accuracy 0.0336 Test Loss 0.018952200150489807\n",
            "Epoch 14 Batch 300 Loss 0.016782273714774074 Accuracy 0.31195494186046513\n",
            "Test Accuracy 0.0373 Test Loss 0.018923163199424744\n",
            "Epoch 15 Batch 0 Loss 0.016303252428770065 Accuracy 0.3828125\n",
            "Test Accuracy 0.0415 Test Loss 0.018903407049179078\n",
            "Epoch 15 Batch 100 Loss 0.016581669357596057 Accuracy 0.33926361386138615\n",
            "Test Accuracy 0.0414 Test Loss 0.018905997490882873\n",
            "Epoch 15 Batch 200 Loss 0.016607319648882644 Accuracy 0.33488805970149255\n",
            "Test Accuracy 0.0382 Test Loss 0.018933948373794557\n",
            "Epoch 15 Batch 300 Loss 0.01663862116189296 Accuracy 0.3302273671096346\n",
            "Test Accuracy 0.0421 Test Loss 0.018912956500053407\n",
            "Epoch 16 Batch 0 Loss 0.016746586188673973 Accuracy 0.3125\n",
            "Test Accuracy 0.0449 Test Loss 0.01889882173538208\n",
            "Epoch 16 Batch 100 Loss 0.016457466408610344 Accuracy 0.356822400990099\n",
            "Test Accuracy 0.0338 Test Loss 0.01896938247680664\n",
            "Epoch 16 Batch 200 Loss 0.016481297887602255 Accuracy 0.35261194029850745\n",
            "Test Accuracy 0.0403 Test Loss 0.018929418683052063\n",
            "Epoch 16 Batch 300 Loss 0.016488601153475106 Accuracy 0.3512510382059801\n",
            "Test Accuracy 0.0403 Test Loss 0.01893934633731842\n",
            "Epoch 17 Batch 0 Loss 0.015880247578024864 Accuracy 0.4375\n",
            "Test Accuracy 0.0345 Test Loss 0.01897429039478302\n",
            "Epoch 17 Batch 100 Loss 0.016337969570909397 Accuracy 0.37414913366336633\n",
            "Test Accuracy 0.0405 Test Loss 0.018932667636871337\n",
            "Epoch 17 Batch 200 Loss 0.016371323620502035 Accuracy 0.3674207089552239\n",
            "Test Accuracy 0.04 Test Loss 0.018924360704421998\n",
            "Epoch 17 Batch 300 Loss 0.016387610755043965 Accuracy 0.36469580564784054\n",
            "Test Accuracy 0.0385 Test Loss 0.018953396248817443\n",
            "Epoch 18 Batch 0 Loss 0.01640648953616619 Accuracy 0.359375\n",
            "Test Accuracy 0.0402 Test Loss 0.018947815012931822\n",
            "Epoch 18 Batch 100 Loss 0.01622990779763106 Accuracy 0.3834313118811881\n",
            "Test Accuracy 0.0398 Test Loss 0.018939749765396117\n",
            "Epoch 18 Batch 200 Loss 0.0162557055339662 Accuracy 0.3814521144278607\n",
            "Test Accuracy 0.0421 Test Loss 0.018929119348526\n",
            "Epoch 18 Batch 300 Loss 0.0162714307577012 Accuracy 0.3799833887043189\n",
            "Test Accuracy 0.042 Test Loss 0.018934289813041686\n",
            "Epoch 19 Batch 0 Loss 0.016665948554873466 Accuracy 0.3125\n",
            "Test Accuracy 0.0371 Test Loss 0.01896817035675049\n",
            "Epoch 19 Batch 100 Loss 0.01610300346913904 Accuracy 0.40238242574257427\n",
            "Test Accuracy 0.0419 Test Loss 0.01893960702419281\n",
            "Epoch 19 Batch 200 Loss 0.016137893524817863 Accuracy 0.3978544776119403\n",
            "Test Accuracy 0.0452 Test Loss 0.01892918915748596\n",
            "Epoch 19 Batch 300 Loss 0.016163698057226367 Accuracy 0.3938693936877076\n",
            "Test Accuracy 0.0381 Test Loss 0.01898062856197357\n",
            "Epoch 20 Batch 0 Loss 0.016133110970258713 Accuracy 0.390625\n",
            "Test Accuracy 0.0381 Test Loss 0.01896777491569519\n",
            "Epoch 20 Batch 100 Loss 0.016019228228026688 Accuracy 0.41336633663366334\n",
            "Test Accuracy 0.0395 Test Loss 0.018948299956321718\n",
            "Epoch 20 Batch 200 Loss 0.016049592931805856 Accuracy 0.4098258706467662\n",
            "Test Accuracy 0.0405 Test Loss 0.018954103970527648\n",
            "Epoch 20 Batch 300 Loss 0.01605209410252464 Accuracy 0.40946843853820597\n",
            "Test Accuracy 0.0457 Test Loss 0.01892970414161682\n",
            "Epoch 21 Batch 0 Loss 0.015870826318860054 Accuracy 0.4453125\n",
            "Test Accuracy 0.0453 Test Loss 0.018941247296333313\n",
            "Epoch 21 Batch 100 Loss 0.015941425918204948 Accuracy 0.42218440594059403\n",
            "Test Accuracy 0.0416 Test Loss 0.018954119205474854\n",
            "Epoch 21 Batch 200 Loss 0.015955334529280663 Accuracy 0.42156405472636815\n",
            "Test Accuracy 0.042 Test Loss 0.018947478485107423\n",
            "Epoch 21 Batch 300 Loss 0.0159742148914854 Accuracy 0.41891611295681064\n",
            "Test Accuracy 0.0432 Test Loss 0.018951699376106263\n",
            "Epoch 22 Batch 0 Loss 0.015324124135077 Accuracy 0.5078125\n",
            "Test Accuracy 0.0456 Test Loss 0.018917244362831116\n",
            "Epoch 22 Batch 100 Loss 0.01585256336902333 Accuracy 0.4345606435643564\n",
            "Test Accuracy 0.0388 Test Loss 0.018986618828773498\n",
            "Epoch 22 Batch 200 Loss 0.015832125806986397 Accuracy 0.43703358208955223\n",
            "Test Accuracy 0.042 Test Loss 0.018957747411727906\n",
            "Epoch 22 Batch 300 Loss 0.015866572594276297 Accuracy 0.4324906561461794\n",
            "Test Accuracy 0.0399 Test Loss 0.01897472577095032\n",
            "Epoch 23 Batch 0 Loss 0.015821266919374466 Accuracy 0.4375\n",
            "Test Accuracy 0.0419 Test Loss 0.01896054048538208\n",
            "Epoch 23 Batch 100 Loss 0.015701804918670417 Accuracy 0.45397586633663367\n",
            "Test Accuracy 0.0421 Test Loss 0.018967204523086548\n",
            "Epoch 23 Batch 200 Loss 0.015737742264361226 Accuracy 0.44935478855721395\n",
            "Test Accuracy 0.0442 Test Loss 0.018936832857131956\n",
            "Epoch 23 Batch 300 Loss 0.015783146506429115 Accuracy 0.44333990863787376\n",
            "Test Accuracy 0.0392 Test Loss 0.018974833178520202\n",
            "Epoch 24 Batch 0 Loss 0.016019929200410843 Accuracy 0.4140625\n",
            "Test Accuracy 0.0429 Test Loss 0.01895845146179199\n",
            "Epoch 24 Batch 100 Loss 0.015646770541178117 Accuracy 0.46217512376237624\n",
            "Test Accuracy 0.041 Test Loss 0.018971847724914552\n",
            "Epoch 24 Batch 200 Loss 0.015700451913868905 Accuracy 0.4541355721393035\n",
            "Test Accuracy 0.0435 Test Loss 0.018957444500923157\n",
            "Epoch 24 Batch 300 Loss 0.015706832820790946 Accuracy 0.45296926910299\n",
            "Test Accuracy 0.0429 Test Loss 0.018958016443252564\n",
            "Epoch 25 Batch 0 Loss 0.01458008587360382 Accuracy 0.609375\n",
            "Test Accuracy 0.0383 Test Loss 0.019001158261299132\n",
            "Epoch 25 Batch 100 Loss 0.015591846889641025 Accuracy 0.468595297029703\n",
            "Test Accuracy 0.0377 Test Loss 0.018996803855895996\n",
            "Epoch 25 Batch 200 Loss 0.015622958679919812 Accuracy 0.4642412935323383\n",
            "Test Accuracy 0.0417 Test Loss 0.018971812534332274\n",
            "Epoch 25 Batch 300 Loss 0.01565944166402286 Accuracy 0.4588870431893688\n",
            "Test Accuracy 0.0445 Test Loss 0.018955452227592467\n",
            "Epoch 26 Batch 0 Loss 0.015459706075489521 Accuracy 0.484375\n",
            "Test Accuracy 0.0435 Test Loss 0.018958879971504212\n",
            "Epoch 26 Batch 100 Loss 0.015583075232582518 Accuracy 0.4681311881188119\n",
            "Test Accuracy 0.0431 Test Loss 0.018966019010543823\n",
            "Epoch 26 Batch 200 Loss 0.01558789176708875 Accuracy 0.46746735074626866\n",
            "Test Accuracy 0.0424 Test Loss 0.018967276406288147\n",
            "Epoch 26 Batch 300 Loss 0.015611346891502606 Accuracy 0.4642078488372093\n",
            "Test Accuracy 0.0469 Test Loss 0.018937858629226683\n",
            "Epoch 27 Batch 0 Loss 0.015382389537990093 Accuracy 0.5\n",
            "Test Accuracy 0.0436 Test Loss 0.01896165511608124\n",
            "Epoch 27 Batch 100 Loss 0.015585640556003788 Accuracy 0.46797648514851486\n",
            "Test Accuracy 0.0401 Test Loss 0.018982673835754395\n",
            "Epoch 27 Batch 200 Loss 0.015551132641139612 Accuracy 0.4728311567164179\n",
            "Test Accuracy 0.0431 Test Loss 0.018964962744712828\n",
            "Epoch 27 Batch 300 Loss 0.015545371443379757 Accuracy 0.4736295681063123\n",
            "Test Accuracy 0.0448 Test Loss 0.018958599185943604\n",
            "Epoch 28 Batch 0 Loss 0.015382434241473675 Accuracy 0.4921875\n",
            "Test Accuracy 0.0438 Test Loss 0.01896162521839142\n",
            "Epoch 28 Batch 100 Loss 0.015480936984393266 Accuracy 0.48004331683168316\n",
            "Test Accuracy 0.0423 Test Loss 0.018983260679244995\n",
            "Epoch 28 Batch 200 Loss 0.015462495131760984 Accuracy 0.48258706467661694\n",
            "Test Accuracy 0.0423 Test Loss 0.0189725665807724\n",
            "Epoch 28 Batch 300 Loss 0.01547504739953434 Accuracy 0.481234426910299\n",
            "Test Accuracy 0.0481 Test Loss 0.018938497757911683\n",
            "Epoch 29 Batch 0 Loss 0.015393105335533619 Accuracy 0.4765625\n",
            "Test Accuracy 0.0412 Test Loss 0.01897639675140381\n",
            "Epoch 29 Batch 100 Loss 0.015421054943805874 Accuracy 0.48901608910891087\n",
            "Test Accuracy 0.0441 Test Loss 0.018966401648521425\n",
            "Epoch 29 Batch 200 Loss 0.015404671537157019 Accuracy 0.4910603233830846\n",
            "Test Accuracy 0.0453 Test Loss 0.01895964343547821\n",
            "Epoch 29 Batch 300 Loss 0.01543333309631411 Accuracy 0.4867109634551495\n",
            "Test Accuracy 0.0445 Test Loss 0.018953645253181457\n",
            "Epoch 30 Batch 0 Loss 0.015576212666928768 Accuracy 0.4609375\n",
            "Test Accuracy 0.0427 Test Loss 0.018977644634246827\n",
            "Epoch 30 Batch 100 Loss 0.015342223522539186 Accuracy 0.49860767326732675\n",
            "Test Accuracy 0.0425 Test Loss 0.018974633646011353\n",
            "Epoch 30 Batch 200 Loss 0.01535070145882629 Accuracy 0.4979011194029851\n",
            "Test Accuracy 0.0451 Test Loss 0.01895721468925476\n",
            "Epoch 30 Batch 300 Loss 0.015380916810684228 Accuracy 0.49390053986710963\n",
            "Test Accuracy 0.0423 Test Loss 0.01898264772891998\n",
            "Epoch 31 Batch 0 Loss 0.01488809660077095 Accuracy 0.5625\n",
            "Test Accuracy 0.0417 Test Loss 0.018988170742988586\n",
            "Epoch 31 Batch 100 Loss 0.015249929049670105 Accuracy 0.5091274752475248\n",
            "Test Accuracy 0.0429 Test Loss 0.018972120451927185\n",
            "Epoch 31 Batch 200 Loss 0.015289655965359058 Accuracy 0.5042754975124378\n",
            "Test Accuracy 0.0409 Test Loss 0.018986632800102234\n",
            "Epoch 31 Batch 300 Loss 0.015315307205252078 Accuracy 0.5011679817275747\n",
            "Test Accuracy 0.0454 Test Loss 0.01896951470375061\n",
            "Epoch 32 Batch 0 Loss 0.015386476181447506 Accuracy 0.4921875\n",
            "Test Accuracy 0.042 Test Loss 0.0189903516292572\n",
            "Epoch 32 Batch 100 Loss 0.015282438874318459 Accuracy 0.5060334158415841\n",
            "Test Accuracy 0.0434 Test Loss 0.01897345802783966\n",
            "Epoch 32 Batch 200 Loss 0.015270016248805902 Accuracy 0.5075792910447762\n",
            "Test Accuracy 0.0474 Test Loss 0.018954276728630065\n",
            "Epoch 32 Batch 300 Loss 0.015289730732746892 Accuracy 0.504749792358804\n",
            "Test Accuracy 0.0489 Test Loss 0.018936938428878784\n",
            "Epoch 33 Batch 0 Loss 0.014189747162163258 Accuracy 0.6484375\n",
            "Test Accuracy 0.0436 Test Loss 0.01898151273727417\n",
            "Epoch 33 Batch 100 Loss 0.015209574012620615 Accuracy 0.515315594059406\n",
            "Test Accuracy 0.0445 Test Loss 0.01897427055835724\n",
            "Epoch 33 Batch 200 Loss 0.015207613484739368 Accuracy 0.5152751865671642\n",
            "Test Accuracy 0.045 Test Loss 0.018964452624320983\n",
            "Epoch 33 Batch 300 Loss 0.015221787669210141 Accuracy 0.5135745431893688\n",
            "Test Accuracy 0.0454 Test Loss 0.018970444560050963\n",
            "Epoch 34 Batch 0 Loss 0.015420930460095406 Accuracy 0.4921875\n",
            "Test Accuracy 0.0436 Test Loss 0.018980669164657592\n",
            "Epoch 34 Batch 100 Loss 0.015213516845118881 Accuracy 0.5164758663366337\n",
            "Test Accuracy 0.0473 Test Loss 0.01894811611175537\n",
            "Epoch 34 Batch 200 Loss 0.015233788750509717 Accuracy 0.5125544154228856\n",
            "Test Accuracy 0.0443 Test Loss 0.01896944603919983\n",
            "Epoch 34 Batch 300 Loss 0.015241105304381181 Accuracy 0.5118095930232558\n",
            "Test Accuracy 0.0431 Test Loss 0.018976686191558838\n",
            "Epoch 35 Batch 0 Loss 0.015261813998222351 Accuracy 0.515625\n",
            "Test Accuracy 0.0443 Test Loss 0.018977450466156006\n",
            "Epoch 35 Batch 100 Loss 0.01514749093405386 Accuracy 0.5223545792079208\n",
            "Test Accuracy 0.0431 Test Loss 0.01897804012298584\n",
            "Epoch 35 Batch 200 Loss 0.015170996934546167 Accuracy 0.519550684079602\n",
            "Test Accuracy 0.0438 Test Loss 0.018968116760253905\n",
            "Epoch 35 Batch 300 Loss 0.015171508917405558 Accuracy 0.5199335548172758\n",
            "Test Accuracy 0.0422 Test Loss 0.01899582464694977\n",
            "Epoch 36 Batch 0 Loss 0.01544327661395073 Accuracy 0.484375\n",
            "Test Accuracy 0.0462 Test Loss 0.018973017978668214\n",
            "Epoch 36 Batch 100 Loss 0.015085843566915776 Accuracy 0.5302444306930693\n",
            "Test Accuracy 0.046 Test Loss 0.018967036986351014\n",
            "Epoch 36 Batch 200 Loss 0.015121296404939682 Accuracy 0.5258473258706468\n",
            "Test Accuracy 0.0443 Test Loss 0.01897285487651825\n",
            "Epoch 36 Batch 300 Loss 0.015137921560840354 Accuracy 0.5236970514950167\n",
            "Test Accuracy 0.0457 Test Loss 0.018965625286102294\n",
            "Epoch 37 Batch 0 Loss 0.015041508711874485 Accuracy 0.53125\n",
            "Test Accuracy 0.0441 Test Loss 0.018975936365127562\n",
            "Epoch 37 Batch 100 Loss 0.01509342870979321 Accuracy 0.5275371287128713\n",
            "Test Accuracy 0.0433 Test Loss 0.018976132106781007\n",
            "Epoch 37 Batch 200 Loss 0.015107352193908312 Accuracy 0.5272465796019901\n",
            "Test Accuracy 0.0468 Test Loss 0.01895782709121704\n",
            "Epoch 37 Batch 300 Loss 0.015129110416972954 Accuracy 0.5244497508305648\n",
            "Test Accuracy 0.0413 Test Loss 0.01900073974132538\n",
            "Epoch 38 Batch 0 Loss 0.014765116386115551 Accuracy 0.5703125\n",
            "Test Accuracy 0.0457 Test Loss 0.018966777849197387\n",
            "Epoch 38 Batch 100 Loss 0.015087819999397392 Accuracy 0.528697400990099\n",
            "Test Accuracy 0.0439 Test Loss 0.01897695391178131\n",
            "Epoch 38 Batch 200 Loss 0.01507287825555054 Accuracy 0.5317164179104478\n",
            "Test Accuracy 0.0434 Test Loss 0.018992573809623717\n",
            "Epoch 38 Batch 300 Loss 0.015075825451814455 Accuracy 0.5315355066445183\n",
            "Test Accuracy 0.0449 Test Loss 0.01897249608039856\n",
            "Epoch 39 Batch 0 Loss 0.014663426205515862 Accuracy 0.5859375\n",
            "Test Accuracy 0.0439 Test Loss 0.01898370306491852\n",
            "Epoch 39 Batch 100 Loss 0.015014298278653975 Accuracy 0.5401454207920792\n",
            "Test Accuracy 0.0453 Test Loss 0.01897405846118927\n",
            "Epoch 39 Batch 200 Loss 0.015053538730673825 Accuracy 0.5342817164179104\n",
            "Test Accuracy 0.0411 Test Loss 0.019000225973129274\n",
            "Epoch 39 Batch 300 Loss 0.015052378855249612 Accuracy 0.534546303986711\n",
            "Test Accuracy 0.0442 Test Loss 0.018977166771888734\n",
            "Epoch 40 Batch 0 Loss 0.014844200573861599 Accuracy 0.5625\n",
            "Test Accuracy 0.0452 Test Loss 0.01897539119720459\n",
            "Epoch 40 Batch 100 Loss 0.014969895197318332 Accuracy 0.5456373762376238\n",
            "Test Accuracy 0.0411 Test Loss 0.01900788815021515\n",
            "Epoch 40 Batch 200 Loss 0.014960312196493741 Accuracy 0.5465640547263682\n",
            "Test Accuracy 0.0451 Test Loss 0.018980704975128174\n",
            "Epoch 40 Batch 300 Loss 0.015008103223115304 Accuracy 0.540282392026578\n",
            "Test Accuracy 0.0451 Test Loss 0.01897811803817749\n",
            "Epoch 41 Batch 0 Loss 0.015002737753093243 Accuracy 0.5390625\n",
            "Test Accuracy 0.0466 Test Loss 0.01896427426338196\n",
            "Epoch 41 Batch 100 Loss 0.015012126923123799 Accuracy 0.5401454207920792\n",
            "Test Accuracy 0.0483 Test Loss 0.018955816507339476\n",
            "Epoch 41 Batch 200 Loss 0.015000479893564289 Accuracy 0.5411613805970149\n",
            "Test Accuracy 0.0465 Test Loss 0.01897316892147064\n",
            "Epoch 41 Batch 300 Loss 0.015008178688090703 Accuracy 0.5403343023255814\n",
            "Test Accuracy 0.0412 Test Loss 0.019002167677879333\n",
            "Epoch 42 Batch 0 Loss 0.015252633951604366 Accuracy 0.515625\n",
            "Test Accuracy 0.0473 Test Loss 0.01896319613456726\n",
            "Epoch 42 Batch 100 Loss 0.01489747177329984 Accuracy 0.5543007425742574\n",
            "Test Accuracy 0.0498 Test Loss 0.018942027926445008\n",
            "Epoch 42 Batch 200 Loss 0.014925300046356756 Accuracy 0.5509950248756219\n",
            "Test Accuracy 0.0454 Test Loss 0.018978781390190123\n",
            "Epoch 42 Batch 300 Loss 0.014942993666741539 Accuracy 0.5489514119601329\n",
            "Test Accuracy 0.0444 Test Loss 0.01899082291126251\n",
            "Epoch 43 Batch 0 Loss 0.014767666347324848 Accuracy 0.5703125\n",
            "Test Accuracy 0.0477 Test Loss 0.018967513036727904\n",
            "Epoch 43 Batch 100 Loss 0.014900368951851189 Accuracy 0.5529084158415841\n",
            "Test Accuracy 0.0481 Test Loss 0.01896067295074463\n",
            "Epoch 43 Batch 200 Loss 0.01491224734733502 Accuracy 0.5519667288557214\n",
            "Test Accuracy 0.0442 Test Loss 0.018979925680160523\n",
            "Epoch 43 Batch 300 Loss 0.014923196360221337 Accuracy 0.5506644518272426\n",
            "Test Accuracy 0.0432 Test Loss 0.01899778263568878\n",
            "Epoch 44 Batch 0 Loss 0.014870607294142246 Accuracy 0.5546875\n",
            "Test Accuracy 0.0435 Test Loss 0.019002161478996277\n",
            "Epoch 44 Batch 100 Loss 0.014892936020278105 Accuracy 0.5553836633663366\n",
            "Test Accuracy 0.0432 Test Loss 0.01900281331539154\n",
            "Epoch 44 Batch 200 Loss 0.01490512993464719 Accuracy 0.5534825870646766\n",
            "Test Accuracy 0.0452 Test Loss 0.018973777866363527\n",
            "Epoch 44 Batch 300 Loss 0.014914889439346386 Accuracy 0.5526889534883721\n",
            "Test Accuracy 0.0519 Test Loss 0.018931372332572936\n",
            "Epoch 45 Batch 0 Loss 0.01547611691057682 Accuracy 0.46875\n",
            "Test Accuracy 0.042 Test Loss 0.018999871373176574\n",
            "Epoch 45 Batch 100 Loss 0.014880156965170166 Accuracy 0.5559251237623762\n",
            "Test Accuracy 0.0461 Test Loss 0.018984487748146057\n",
            "Epoch 45 Batch 200 Loss 0.014897694042072961 Accuracy 0.5540267412935324\n",
            "Test Accuracy 0.0468 Test Loss 0.018969404172897338\n",
            "Epoch 45 Batch 300 Loss 0.014893780490290684 Accuracy 0.5544539036544851\n",
            "Test Accuracy 0.0464 Test Loss 0.018979196882247924\n",
            "Epoch 46 Batch 0 Loss 0.014425033703446388 Accuracy 0.625\n",
            "Test Accuracy 0.0444 Test Loss 0.01899066197872162\n",
            "Epoch 46 Batch 100 Loss 0.014857319930549895 Accuracy 0.5590965346534653\n",
            "Test Accuracy 0.0477 Test Loss 0.018968058967590334\n",
            "Epoch 46 Batch 200 Loss 0.014830829825863909 Accuracy 0.562461131840796\n",
            "Test Accuracy 0.0461 Test Loss 0.018985637235641478\n",
            "Epoch 46 Batch 300 Loss 0.014862446052538992 Accuracy 0.5581914451827242\n",
            "Test Accuracy 0.0496 Test Loss 0.018939593029022216\n",
            "Epoch 47 Batch 0 Loss 0.01471320353448391 Accuracy 0.5703125\n",
            "Test Accuracy 0.0459 Test Loss 0.018983469438552857\n",
            "Epoch 47 Batch 100 Loss 0.014852266601923078 Accuracy 0.5598700495049505\n",
            "Test Accuracy 0.0439 Test Loss 0.018991447472572327\n",
            "Epoch 47 Batch 200 Loss 0.014854508354815084 Accuracy 0.5601290422885572\n",
            "Test Accuracy 0.0447 Test Loss 0.018994793033599855\n",
            "Epoch 47 Batch 300 Loss 0.014852986117187529 Accuracy 0.5599304401993356\n",
            "Test Accuracy 0.0476 Test Loss 0.01896802294254303\n",
            "Epoch 48 Batch 0 Loss 0.014201290905475616 Accuracy 0.640625\n",
            "Test Accuracy 0.0464 Test Loss 0.01897703347206116\n",
            "Epoch 48 Batch 100 Loss 0.014774439625884636 Accuracy 0.5691522277227723\n",
            "Test Accuracy 0.0467 Test Loss 0.018974015164375305\n",
            "Epoch 48 Batch 200 Loss 0.014802272385217953 Accuracy 0.5656483208955224\n",
            "Test Accuracy 0.0435 Test Loss 0.018998150658607483\n",
            "Epoch 48 Batch 300 Loss 0.014798405283189493 Accuracy 0.5661856312292359\n",
            "Test Accuracy 0.0472 Test Loss 0.01896699502468109\n",
            "Epoch 49 Batch 0 Loss 0.014798580668866634 Accuracy 0.5703125\n",
            "Test Accuracy 0.0431 Test Loss 0.018991536593437195\n",
            "Epoch 49 Batch 100 Loss 0.014793232867106943 Accuracy 0.5672184405940595\n",
            "Test Accuracy 0.0416 Test Loss 0.01900409436225891\n",
            "Epoch 49 Batch 200 Loss 0.01479324470946 Accuracy 0.5675917288557214\n",
            "Test Accuracy 0.0478 Test Loss 0.018962586522102354\n",
            "Epoch 49 Batch 300 Loss 0.014810022777348658 Accuracy 0.5655886627906976\n",
            "Test Accuracy 0.0429 Test Loss 0.01900123586654663\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "IOCmodel = MLP(IOCflag=True).cuda()\n",
        "optimizer = optim.Adam(IOCmodel.parameters(), lr=0.0001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
        "Ioctrain_loss , Ioctrain_acc , Ioctest_loss , Ioctest_acc = train(IOCmodel , train_loader,corrupted_trainloader,scheduler , optimizer , 25 , IOCflag = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cr7beylnFycc",
        "outputId": "304e47b8-f27e-484d-b85e-fbff2fd24a2f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Batch 0 Loss 0.017869671806693077 Accuracy 0.1875\n",
            "Test Accuracy 0.0985 Test Loss 0.018016371388435364\n",
            "Epoch 0 Batch 100 Loss 0.017766732224585986 Accuracy 0.1608910891089109\n",
            "Test Accuracy 0.09252 Test Loss 0.01823573387145996\n",
            "Epoch 0 Batch 200 Loss 0.017758767384646545 Accuracy 0.16215796019900497\n",
            "Test Accuracy 0.09512 Test Loss 0.018169270610809325\n",
            "Epoch 0 Batch 300 Loss 0.017769539351281137 Accuracy 0.16115552325581395\n",
            "Test Accuracy 0.09238 Test Loss 0.018305841755867004\n",
            "Epoch 1 Batch 0 Loss 0.01781630516052246 Accuracy 0.140625\n",
            "Test Accuracy 0.08856 Test Loss 0.018287194395065307\n",
            "Epoch 1 Batch 100 Loss 0.017520985335554226 Accuracy 0.19152227722772278\n",
            "Test Accuracy 0.08932 Test Loss 0.01829835401058197\n",
            "Epoch 1 Batch 200 Loss 0.017527151917714385 Accuracy 0.19640080845771143\n",
            "Test Accuracy 0.08792 Test Loss 0.0183915215921402\n",
            "Epoch 1 Batch 300 Loss 0.017507422956764895 Accuracy 0.2006592607973422\n",
            "Test Accuracy 0.08904 Test Loss 0.018367607927322387\n",
            "Epoch 2 Batch 0 Loss 0.016899973154067993 Accuracy 0.2578125\n",
            "Test Accuracy 0.08676 Test Loss 0.018436910562515257\n",
            "Epoch 2 Batch 100 Loss 0.017470183146029414 Accuracy 0.2088490099009901\n",
            "Test Accuracy 0.08954 Test Loss 0.01843785192966461\n",
            "Epoch 2 Batch 200 Loss 0.017551135559061272 Accuracy 0.20335820895522388\n",
            "Test Accuracy 0.08694 Test Loss 0.018506200470924376\n",
            "Epoch 2 Batch 300 Loss 0.017579040326103815 Accuracy 0.2011264534883721\n",
            "Test Accuracy 0.08624 Test Loss 0.018469639768600464\n",
            "Epoch 3 Batch 0 Loss 0.017132436856627464 Accuracy 0.265625\n",
            "Test Accuracy 0.08778 Test Loss 0.018481005611419677\n",
            "Epoch 3 Batch 100 Loss 0.017383299140941978 Accuracy 0.22988861386138615\n",
            "Test Accuracy 0.08368 Test Loss 0.018500522994995116\n",
            "Epoch 3 Batch 200 Loss 0.017368868762506776 Accuracy 0.23157649253731344\n",
            "Test Accuracy 0.08338 Test Loss 0.018486641573905945\n",
            "Epoch 3 Batch 300 Loss 0.01733472270649731 Accuracy 0.2349719684385382\n",
            "Test Accuracy 0.0824 Test Loss 0.018489966607093813\n",
            "Epoch 4 Batch 0 Loss 0.016857178881764412 Accuracy 0.296875\n",
            "Test Accuracy 0.08046 Test Loss 0.018512317504882813\n",
            "Epoch 4 Batch 100 Loss 0.01706835295599286 Accuracy 0.26895111386138615\n",
            "Test Accuracy 0.0807 Test Loss 0.01852662305831909\n",
            "Epoch 4 Batch 200 Loss 0.017081614722734065 Accuracy 0.26830690298507465\n",
            "Test Accuracy 0.07946 Test Loss 0.01853647032737732\n",
            "Epoch 4 Batch 300 Loss 0.017085441484106737 Accuracy 0.2679609634551495\n",
            "Test Accuracy 0.08048 Test Loss 0.01853789249420166\n",
            "Epoch 5 Batch 0 Loss 0.01676871068775654 Accuracy 0.3203125\n",
            "Test Accuracy 0.08018 Test Loss 0.018540299282073975\n",
            "Epoch 5 Batch 100 Loss 0.017023101301476506 Accuracy 0.278697400990099\n",
            "Test Accuracy 0.08062 Test Loss 0.018554647235870362\n",
            "Epoch 5 Batch 200 Loss 0.017019070008426757 Accuracy 0.2788790422885572\n",
            "Test Accuracy 0.07894 Test Loss 0.01856183060646057\n",
            "Epoch 5 Batch 300 Loss 0.017013391547919903 Accuracy 0.2795888704318937\n",
            "Test Accuracy 0.0793 Test Loss 0.018561777067184447\n",
            "Epoch 6 Batch 0 Loss 0.017480628564953804 Accuracy 0.21875\n",
            "Test Accuracy 0.0805 Test Loss 0.018547285995483397\n",
            "Epoch 6 Batch 100 Loss 0.01692130892436103 Accuracy 0.29347153465346537\n",
            "Test Accuracy 0.07822 Test Loss 0.018578927993774415\n",
            "Epoch 6 Batch 200 Loss 0.016929153362242738 Accuracy 0.29221082089552236\n",
            "Test Accuracy 0.0795 Test Loss 0.018580361862182617\n",
            "Epoch 6 Batch 300 Loss 0.016932226661778366 Accuracy 0.2913984634551495\n",
            "Test Accuracy 0.07736 Test Loss 0.018600815768241883\n",
            "Epoch 7 Batch 0 Loss 0.016559911891818047 Accuracy 0.34375\n",
            "Test Accuracy 0.07742 Test Loss 0.018599838852882387\n",
            "Epoch 7 Batch 100 Loss 0.016845131532685593 Accuracy 0.3015934405940594\n",
            "Test Accuracy 0.0764 Test Loss 0.018595675678253175\n",
            "Epoch 7 Batch 200 Loss 0.016861676372847155 Accuracy 0.29971237562189057\n",
            "Test Accuracy 0.07562 Test Loss 0.018602600917816164\n",
            "Epoch 7 Batch 300 Loss 0.01682695831818834 Accuracy 0.30385693521594687\n",
            "Test Accuracy 0.07574 Test Loss 0.018610875873565674\n",
            "Epoch 8 Batch 0 Loss 0.01668749563395977 Accuracy 0.328125\n",
            "Test Accuracy 0.07544 Test Loss 0.01861302276134491\n",
            "Epoch 8 Batch 100 Loss 0.01668934158907078 Accuracy 0.32062190594059403\n",
            "Test Accuracy 0.07446 Test Loss 0.01862199785232544\n",
            "Epoch 8 Batch 200 Loss 0.016691823021985998 Accuracy 0.3204679726368159\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-b4ababbc35eb>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIOCmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mIoctrain_loss\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mIoctrain_acc\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mIoctest_loss\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mIoctest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIOCmodel\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcorrupted_trainloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mIOCflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-59-9d39dd8bbf12>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, scheduler, optimizer, epochs, IOCflag)\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mtrain_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_correct\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mtest_acc\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0mtest_losslst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mtest_acclst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-9d39dd8bbf12>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, test_loader, criterion)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0;31m# print(len(data))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2854\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2856\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2794\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2796\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombytes\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2741\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2742\u001b[0;31m     \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2743\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the loss and accuracy curves\n",
        "\n",
        "def plot_loss(train_loss , test_loss , name):\n",
        "    plt.plot(train_loss , label = \"Train Loss\")\n",
        "    plt.plot(test_loss , label = \"Test Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Loss vs Epochs for \" + name)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_acc(train_acc , test_acc , name):\n",
        "    plt.plot(train_acc , label = \"Train Accuracy\")\n",
        "    plt.plot(test_acc , label = \"Test Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Accuracy vs Epochs for \" + name)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_loss(MLPtrain_loss , MLPtest_loss , \"MLP\")\n",
        "plot_loss(Ioctrain_loss , Ioctest_loss , \"MLP with IOC\")\n",
        "\n"
      ],
      "metadata": {
        "id": "3lc7DLr9nHj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_acc(MLPtrain_acc , MLPtest_acc , \"MLP\")\n",
        "plot_acc(Ioctrain_acc , Ioctest_acc , \"MLP with IOC\")"
      ],
      "metadata": {
        "id": "U4KJwTCJnNA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# save the model\n",
        "path = \"/content/drive/My Drive/\"\n",
        "torch.save(MLPmodel.state_dict(), path + 'MLPmodel.pt')\n",
        "torch.save(IOCmodel.state_dict(), path + 'IOCmodel.pt')\n",
        "\n",
        "# save the loss and accuracy\n",
        "MLPdata = {\"train_loss\" : MLPtrain_loss , \"train_acc\" : MLPtrain_acc , \"test_loss\" : MLPtest_loss , \"test_acc\" : MLPtest_acc}\n",
        "IOCdata = {\"train_loss\" : Ioctrain_loss , \"train_acc\" : Ioctrain_acc , \"test_loss\" : Ioctest_loss , \"test_acc\" : Ioctest_acc}\n",
        "with open(path + 'MLPdata.json', 'w') as fp:\n",
        "    json.dump(MLPdata, fp)\n",
        "with open(path + 'IOCdata.json', 'w') as fp:\n",
        "    json.dump(IOCdata, fp)\n",
        "    "
      ],
      "metadata": {
        "id": "dgAGV-aglPBu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}